{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import scipy.sparse as sp\n",
    "from input_data import load_data\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "def ismember(a, b, tol=5):\n",
    "    rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "    return np.any(rows_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_link(dataset,epochs):\n",
    "    #load samples\n",
    "    \n",
    "    adj, features,adj_train, val_edges, val_edges_false, test_edges, test_edges_false,labels = load_data(dataset)\n",
    "    adj_tuple = sparse_to_tuple(adj)\n",
    "    adj_train_tuple = sparse_to_tuple(adj_train)\n",
    "    train_edges_false = np.load('./data/'+dataset+'_train_edges_false.npy')\n",
    "    train_all_edges = np.concatenate((adj_train_tuple[0],train_edges_false),axis=0) \n",
    "    labels=np.zeros(train_all_edges.shape)\n",
    "    labels[:int(train_all_edges.shape[0]/2),0]=1\n",
    "    labels[int(train_all_edges.shape[0]/2):,1]=1\n",
    "    permutation = np.random.permutation(train_all_edges.shape[0])\n",
    "    train_all_edges = train_all_edges[permutation,:]\n",
    "    labels = labels[permutation,:]\n",
    "    \n",
    "    #load_embeddings\n",
    "    emb = np.load('./data/'+dataset+'_emb.npy')\n",
    "\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    x1 = tf.placeholder('float', [None, 64])\n",
    "    x2 = tf.placeholder('float', [None, 64])\n",
    "    y = tf.placeholder('float', [None, 2])\n",
    "\n",
    "\n",
    "    x11 = tf.nn.relu(tf.layers.dense(inputs=x1, units=32))\n",
    "    x21 = tf.nn.relu(tf.layers.dense(inputs=x2, units=32))\n",
    "    x31= tf.concat([x11, x21], 1)\n",
    "    x41 = tf.nn.relu(tf.layers.dense(inputs=x31, units=16))\n",
    "    x4 = tf.nn.relu(tf.layers.dense(inputs=x41, units=8))\n",
    "    preds = tf.layers.dense(inputs=x4, units=2)\n",
    "    cross_entropy = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(logits=preds,multi_class_labels=y))   \n",
    "\n",
    "    sess =  tf.Session()\n",
    "\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate= 0.01).minimize(cross_entropy)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    flag = 0\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if flag*100+100 > train_all_edges.shape[0]:\n",
    "            flag = 0\n",
    "        a = flag*100\n",
    "        b = a+100\n",
    "        flag = flag+1\n",
    "        batch_edges = train_all_edges[a:b,:]\n",
    "        batch_y = labels[a:b]\n",
    "        batch_x1 = emb[batch_edges[:,0],:]\n",
    "        batch_x2 = emb[batch_edges[:,1],:]\n",
    "        _,loss,preds_ = sess.run([train_op, cross_entropy,preds], feed_dict={x1: batch_x1,x2:batch_x2,y:batch_y})\n",
    "\n",
    "#         if epoch%1000 == 0:\n",
    "#             print(epoch)\n",
    "\n",
    "    test_all_edges = np.concatenate((test_edges, test_edges_false),axis=0) \n",
    "    test_labels=np.zeros(test_all_edges.shape)\n",
    "    test_labels[:int(test_all_edges.shape[0]/2),0]=1\n",
    "    test_labels[int(test_all_edges.shape[0]/2):,1]=1\n",
    "    test_preds=np.empty((0,2))\n",
    "    flag=0\n",
    "    for epoch in range(int(test_all_edges.shape[0]/100)):\n",
    "        if flag*100+100 > test_all_edges.shape[0]:\n",
    "            flag = 0\n",
    "        a = flag*100\n",
    "        b = a+100\n",
    "        flag = flag+1\n",
    "        batch_edges = test_all_edges[a:b,:]\n",
    "        batch_y = test_labels[:100,:]\n",
    "        batch_x1 = emb[batch_edges[:,0],:]\n",
    "        batch_x2 = emb[batch_edges[:,1],:]\n",
    "        batch_preds = sess.run(preds, feed_dict={x1: batch_x1,x2:batch_x2,y:batch_y})\n",
    "        test_preds = np.vstack((test_preds,batch_preds))\n",
    "    test_preds.shape\n",
    "    test_labels = test_labels[:int((test_all_edges.shape[0])/100)*100,:]\n",
    "    #p = np.where(test_preds>0)[1]\n",
    "    p=[]\n",
    "    for label in test_preds:\n",
    "        if label[0]>=label[1]:\n",
    "            p.append(0)\n",
    "        else:\n",
    "            p.append(1)\n",
    "        \n",
    "    l = test_labels[:,1]\n",
    "    from sklearn.metrics import f1_score,accuracy_score\n",
    "    acc = accuracy_score(l, p) \n",
    "    f1 = f1_score(l,p,average='macro') \n",
    "    print(acc)\n",
    "    print(f1)\n",
    "\n",
    "    f = open('./data/'+dataset +'_results.txt', 'r+')\n",
    "    content = f.read()\n",
    "    f.seek(0, 0)\n",
    "    f.write(str(acc)+'\\n')\n",
    "    f.write(str(f1)+'\\n'+content)\n",
    "    f.close()\n",
    "    return acc,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    " \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-43e2569168d4>:25: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/khfu/.local/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/khfu/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1632: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "0.8174444444444444\n",
      "0.8169979428266678\n",
      "ACC of link predction:0.8174444444444444\n",
      "Macro F1 of link predction:0.8169979428266678\n"
     ]
    }
   ],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "# Settings\n",
    "flags.DEFINE_string('f', '', 'Kernel')\n",
    "flags.DEFINE_string('dataset', 'yale', 'Name of dateset')\n",
    "dataset = FLAGS.dataset \n",
    "if dataset == 'rochester':\n",
    "    epochs=15000\n",
    "else:\n",
    "    epochs=25000\n",
    "acc,f1 = pred_link(dataset,epochs)\n",
    "print(\"ACC of link predction:\"+ str(acc))\n",
    "print(\"Macro F1 of link predction:\"+ str(f1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
