\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
 \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}
\usepackage{amsmath}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{subfig}

\title{Privacy Preserving Social Network Embedding}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David S.~Hippocampus \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
Graph embedding has been proved extremely useful to learn low-dimensional feature representations from a graph. These feature representations can be used for a variety of prediction tasks from node classification to link prediction. However, existing graph embedding methods do not consider users' privacy in order to prevent inference attacks. That is, adversaries could infer users' sensitive information by analyzing node representations learned from a graph embedding algorithm. In this paper, we propose Adversarial Privacy Graph Embedding (APGE), a graph adversarial training framework that integrates the disentangling and purging mechanisms to remove users' private information from learned node representations. Extensive experiments on real-world social network datasets demonstrate the superior performance of APGE compared to existing graph embedding methods. The proposed method preserves the structural information and utility attributes of a social network while concealing users' private attributes from inference attacks. To the best of our knowledge, this is the first work considering preserving privacy in social network embedding. The code of this work is available at https://github.com/uJ62JHD/Privacy-Preserving-Social-Network-Embedding.
\end{abstract}


%======================================
\section{Introduction}
%Among many big data resources, online social networks (OSNs) contribute considerable amount of data covering all aspects of the frontend and backend. OSN platforms have attracted great attention from the public, for example, Facebook has 1.65 billion users with 1 billion active users per month, Twitter owns 600 million users with 0.5 billion tweets published per day, while WeChat has over a billion users with 700 million active users, etc. With such large scale and variety of data, social network analysis has become increasingly significant for classifying end-users, predicting buying interests, and foretelling event occurrence, etc. With the boom of the popularity, OSNs offer a great opportunity for SNA researcher to prompt novel applications.




%Although abundant social data brings valuable beneﬁts, it also raises stringent privacy concerns as well. Each OSN user is generally associated with a set of attributes, some of, which may have high sensitivity like location, gender, sexual orientation, etc. Such personal information could be exploited by third parties like data analysts, marketer, or social media itself. Any third parties with malicious intentions on sensitive information of users can be viewed as adversaries and they breach user privacy by collecting sensitive data ﬁrst. Concerns derived from inference attacks towards sensitive information contained in user data is represented as latent-data privacy, where the inference attacks usually employ statistical analysis, machine learning or data mining techniques to infer sensitive information. For instance, suppose a user does not disclose her opinions and interests online. It is still easy to predict some of her opinions and interests if it is publicly known that she is afﬁliated with any particular organization or club. ABCNews.com [1] and Boston Globe [2] showed it is achievable to infer the sexual orientation of a user through mining a Facebook subnetwork involving the user’s friendship relations, gender, and other attributes. Data privacy breaches could incur serious negative repercussions.

Recently, graph embedding has made prominent progress in social network analysis~\citep{goyal2018graph,cui2018survey}. It's a technique that learns a low-dimensional representation for each node of a graph based on network topology and nodes' attributes. There are several advantages of using graph embedding for social network analysis: (1) through this technology, the downstream applications, such as node classification, clustering, link prediction and recommendation, can be performed on the learned node representations directly. (2) Many graph mining algorithms contain iterative or combinatorial computations, whose complexities are typically NP-hard. By graph embedding, the computation of these tasks can be reduced dramatically. (3) It's very challenging to design parallel or distributed algorithms on graph data directly; more efficient parallel and distributed algorithms can be developed readily on learned node representations. However, the existing graph embedding algorithms do not consider users' privacy to prevent inference attacks. That is, from the learned node representation, adversaries can infer the sensitive information that users have no intent to disclose originally.


On one hand, social networks (e.g. Facebook, Twitter, WeChat) have became important platforms for people to interact with others and to access social media information. Users can freely build linkages and publish their profiles on social networks, which can be exploited by third party analysts to predict users' purchase interest, discover trending events and so on. However, the third parties may also use the sensitive information, such as sexual orientation and political tendency, from social networks with malicious intents. Worse, even if these sensitive attributes are deleted from social networks, adversaries can still mine the privacy information via inference attacks~\citep{lindamood2009inferring}. For examples, users' sexual orientation could be inferred based on their Facebook data, such as users' linkages, genders, and other attributes~\cite{jernigan2009gaydar}; and social network topology and users' behavior data can be used to infer users' geolocations~\cite{gong2018attribute}. All of these demonstrate that our social networks are facing a serious privacy issue. Therefore, when social networks publish their data, they must preprocess their data to prevent inference attacks.



On the other hand, existing privacy protection methods on social networks prevent inference attacks by pruning the graph data directly~\citep{cai2018collective,he2018latent}. The subsequent graph embedding on such preprocessed networks will induce suboptimal performance for privacy protection and data utility reservation. This is because:
(1) Graph embedding and privacy protection are decoupled in this process and limit the expressiveness of the model. (2) Existing methods have limited options to preprocess users' attributes. For example, we can either swap gender attribute of a user or delete it from the node completely to conceal users' private information. With graph embedding, users' gender information will be embedded in a continuous space and therefore we can fine-tune the latent space through gradient updates to protect users' privacy precisely. (3) Traditional social network privacy-preserving methods only consider the first-order friendship, while graph embedding concerns global graph topology and local node attributes. Therefore, high-order friendship can also be used to infer users' sensitive information. For instance, two users don't have direct connections, but they may have shared friends; if only the first-order friendship is utilized, these two users might be considered irrelevant; but they are closely related to each other due to the high-order friendship, which means their private attributes might be similar with a high probability.

Therefore, in this paper we propose a privacy-preserving social network embedding framework, which integrates graph embedding and privacy protection into an end-to-end pipeline to overcome the aforementioned limitations. As we will demonstrate, our method enables social network websites to release their privacy-preserved node embedding to the third parties with reduced risks of privacy concern. The contributions of this paper are
\begin{itemize}
\item To the best of our knowledge, this is the first social network embedding algorithm considering privacy preserving.
\item We propose Adversarial Privacy Graph Embedding (APGE), an adversarial training based graph embedding algorithm that preserves users' privacy in an end-to-end graph convolution pipeline.
\item Extensive experiments on social network datasets demonstrate the superior performance of our method over previous approaches in terms of privacy protection and users' utility reservation.
\end{itemize}


%======================================
\section{Preliminary}
A social network can be represented by a graph $\mathbf{G}(\mathbf{V},\mathbf{E},\mathbf{X})$, where $\mathbf{V}$ denotes the set of $N$ nodes, $\mathbf{E}$ denotes the set of all edges, and $\mathbf{X} \in \mathbb{R}^{N \times P}$ is the attribute matrix, each row of which representing the feature vector of a node. Moreover, we introduce $\mathbf{A} \in \mathbb{R}^{N \times N}$ as the adjacent  matrix  of $\mathbf{G}$. In this paper, we use capital variables (e.g., $\mathbf{X}$) to denote matrices and lower-case variables (e.g., $\mathbf{x} $) to denote row vectors. Therefore, we use $\mathbf{x}_i$ to denote the $i$-th row of the matrix $\mathbf{X}$, where each element of $\mathbf{x}_i$ denotes one attribute of node $i$.

We aim to represent each node $\mathbf{v}_i \in \mathbf{V}$ by a low-dimensional dense vector $\mathbf{z}_i\in\mathbb{R}^{D}$. The learned node representations $\mathbf{Z}$ should satisfy two properties: (1) The network structure and node utility attributes are preserved; and (2) the sensitive information are concealed and robust to inference attacks. That is, if we use the learned node representation matrix $\mathbf{Z}$ to analyze social network, the accuracies of link prediction and utility classification should be preserved, while the accuracy of privacy inference should be reduced.


%======================================
\subsection{Graph Autoencoder}
Graph autoencoder (GAE)~\citep{kipf2016variational} embeds a graph $\mathbf{G}(\mathbf{V},\mathbf{E},\mathbf{X})$ to a low-dimensional space. The encoder of GAE is a graph convolutional network (GCN)~\citep{kipf2016semi}, which updates hidden layer of a graph by
\begin{equation}
\mathbf{H}^{(l+1)} = \sigma(\mathbf{L}\mathbf{H}^{(l)}\mathbf{W}^{(l)}),
\end{equation}
where $\mathbf{L}=\mathbf{D}^{-\frac{1}{2}}\mathbf{\widetilde{A}}\mathbf{D}^{-\frac{1}{2}}$ is the symmetrically normalized graph Laplacian, $\mathbf{H}^{(l)}$ is the output of $l$-th graph convolutional layer, $\mathbf{W}^{(l)}$ is the weight matrix of $l$-th layer that is to be learned during training, and $\sigma$ represents the activation function, such as ReLU. Additionally, $\mathbf{\widetilde{A}}$ denotes the adjacency matrix $\mathbf{A}$ with diagonal element set to 1, i.e., every node contains a self-loop, and $\mathbf{D}_{ii}=\sum_j \mathbf{\widetilde{A}}_{ij} $ denotes the degree of node $i$. Usually, we stack two graph convolutional layers as the encoder, with the propagation rule of the encoder expressed as follows:
\begin{equation}
 GCN(\mathbf{A},\mathbf{X}) = \mathbf{L}\sigma(\mathbf{L}\mathbf{X}\mathbf{W}^{(0)})\mathbf{W}^{(1)}.
\label{Eq:GCN}
\end{equation}

The purpose of GAE is to embed the structural information of a graph to a low-dimensional space. Therefore, the decoder of GAE reconstructs adjacency matrix via the inner product of embedding matrix $\mathbf{Z}$.
The reconstructs adjacency matrix $\mathbf{\hat{A}}$ and the embedding matrix $\mathbf{Z}$ can be represented as follows:
\begin{equation}
\mathbf{\hat{A}} =\sigma(\mathbf{ZZ}^T),  \quad \text{with} \quad   \mathbf{Z}=GCN(\mathbf{A},\mathbf{X}).
\label{Eq:innerP}
\end{equation}

To preserve the structural information of a graph, GAE optimizes the link prediction by minimizing the cross-entropy loss:
\begin{equation}
L_{link} = -\frac{1}{N^2}\sum_{i=1}^N \sum_{j=1}^N  \mathbf{A}_{ij} \log(\mathbf{\hat{A}}_{ij}),
\label{Eq:LossLink}
\end{equation}
where $\mathbf{A}_{ij}$ and $\mathbf{\hat{A}}_{ij}$ are the elements of  $\mathbf{A}$ and $\mathbf{\hat{A}}$, respectively.


%======================================
\subsection{Adversarial Autoencoder}\label{sec:AAE}
Adversarial Autoencoder (AAE)~\citep{makhzani2015adversarial} is a probabilistic autoencoder which uses the generative adversarial network (GAN) to perform variational inference by enforcing the posterior distribution of the hidden code to a specified  prior distribution. The classical AAE architecture is shown in Fig.~\ref{Fig:AAE}\subref{Classcial AAE}. The top row is an autoencoder which encodes the $\mathbf{U}$ to hidden representation $\mathbf{Z}$ and decodes $\mathbf{Z}$ to a reconstructed data $\mathbf{\hat{U}}$. The bottom row is a discriminator which classifies the input is the code $\mathbf{z_i}$ or the noise sampled from the specified prior distribution. As we train AAE, we update the encoder and decoder to minimize the reconstruction error firstly. Then we update the discriminator to distinguish if the input is from true sample or the code generator, and we update the encoder to fool the discriminator.

In \cite{makhzani2015adversarial}, the above classic AAE has been further extended to supervised AAE which provides a one-hot encoding of the label to the decoder as shown in  Fig.~\ref{Fig:AAE}\subref{Supervised AAE}. On the one hand, the distribution of $\mathbf{Z}$ is regularized to the specified  prior distribution. So the information of $\mathbf{Z}$ should be as little as possible. On the other hand, $\mathbf{Z}$ should contain the information of $\mathbf{U}$ as much as possible to reconstruct $\mathbf{U}$. To achieve this, the label information can be disentangled from the latent representation $\mathbf{Z}$ when the class label is input to decoder. In this paper, such a characteristic of supervised AAE is exploited for privacy protection.
%\begin{figure}
%  \centering
%  \includegraphics[width=0.5\linewidth]{AAE.eps}
%  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%  \caption{Sample figure caption.}
%\end{figure}
\begin{figure}[!ht]
\vspace{-15pt}
     \subfloat[Classcial AAE\label{Classcial AAE}]{%
       \includegraphics[width=0.35\textwidth]{AAE.pdf}
     }
     \hfill
     \subfloat[Supervised AAE\label{Supervised AAE that disentangles lable information from the latent representation}]{%
       \includegraphics[width=0.35\textwidth]{surAAE.pdf}
     }
     \caption{Adversarial Autoencoder}
     \label{Fig:AAE}
   \end{figure}


%======================================
\section{Proposed Model}
We first discuss two privacy preserving embedding models we develop from different perspectives. Then we integrate them into one end-to-end pipeline to achieve the best performance. In the sequel, we will introduce these models one by one.


%======================================
\subsection{Adversarial Privacy-Disentangled Graph Embedding}
As indicated in section~\ref{sec:AAE}, supervised AAE can be readily adapted for privacy protection. The main issue is that the input of supervised AAE is regular grid data, such as images, while we are interested in social network data which are irregular graph structured data. We therefore extend GAE and supervised AAE and propose Adversarial Privacy-Disentangled Graph Embedding (APDGE) for privacy preserving.

As shown in Fig.~\ref{Fig:ASGE}, the architecture of APDGE is similar to supervised AAE that consists of encoder, decoder and discriminator. To support graph data, the encoder of AAE is replaced by GCN, that is, latent code $\mathbf{Z'}$ is extracted by a GCN: $GCN(\mathbf{A},\mathbf{X})$ as shown in Eq.~\ref{Eq:GCN}. To disentangle privacy from latent code $\mathbf{Z'}$, we incorporate the privacy label (e.g., gender) to the decoder such that the model can learn a compressed $\mathbf{Z'}$ that is sufficient for the decoder to reconstruct the graph data with the help of explicit privacy label encoding.

We hope the dimension of $\mathbf{Z}'$ is low so that the privacy could be extruded as much as possible. On the other hand, if the dimension of $\mathbf{Z}'$ is too low, the performance will be suffering. From our experiment, we find the following trick enables us to meet the both goals. That is, before feeding to the decoder, we map the low dimensional representation $\mathbf{Z}'$ to a higher dimensional $\mathbf{Z}$ via a fully connected layer; then we concatenate the one-hot privacy label encoding with $\mathbf{Z}$ to get $\mathbf{Z^+}$, which is the input to the decoder.

Because the embedding result should retain graph topology and utility attributes information, the decoder contains multiple modules for adjacency matrix reconstruction and attribute classifiers. The adjacency reconstruction module calculates the inner production of $\mathbf{Z^+}$ to reconstruct adjacency matrix with the loss function for link prediction as Eq. \ref{Eq:LossLink}. For utility attribute classification, we decode $\mathbf{Z^+}$ using the softmax function, followed by the cross-entropy loss. Precisely, we predict the $i$-th user's $c$-th utility attribute by $\mathbf{\hat{y}}_i^{(c)} = softmax_c(\mathbf{z}_i^+)$, which is evaluated by the cross-entropy loss as:
\begin{equation}
L_{y_c} = -\frac{1}{|\mathbf{V}^{(c)}|}\sum_{i \in \mathbf{V}^{(c)}}\sum_{j=1}^{M_c}\mathbf{y}^{(c)}_{ij}\log\mathbf{\hat{y}}^{(c)}_{ij},
\label{Eq:LossAttr}
\end{equation}
where $\mathbf{V}^{(c)}$ is the set of users on which the $c$-th utility labels are available for training, and  $M_c$ is the dimension of the  $c$-th utility attribute. The total loss of ADPGE is the combination of the link prediction loss and utility classification loss:
\begin{equation}\label{Eq:loss_rec}
L_{recon} = L_{link} + \lambda\sum_{c \in \mathbf{C}} L_{y_c},
\end{equation}
where $\mathbf{C}$ is the set of  utility attributes, and $\lambda$ is a trade-off parameter between the link prediction loss and the utility classification losses, and in our experiment we set $\lambda=1$ by default.

The discriminator $D$ of this model is the same as the discriminator of AAE. We use two fully connected layers to classify the input to be real or fake depending on if the input $\mathbf{s}$ is sampled from a unit Gaussian distribution or from the encoder of ADPGE. We optimize the discriminator to minimize the following loss:
\begin{equation}
L_{dc} = - \log(D(\mathbf{s})) - \log(1-D(\mathbf{z'_i})).
\label{Eq:LossDc}
\end{equation}

Overall, the training of APDGE contains three stages: (a) Update the encoder and decoder to minimize the loss Eq.~\ref{Eq:loss_rec}, (b) Update the discriminator to distinguish the real samples from the fake samples by minimizing the loss Eq.~\ref{Eq:LossDc}, and (c) Update the encoder to fool the discriminator by maximizing the loss Eq.~\ref{Eq:LossDc}. Upon finishing the training, we get the privacy-preserved node representation (latent code) matrix $\mathbf{Z}$.
\begin{figure}[htbp]
%\vspace{-15pt}
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering \includegraphics[width=6cm]{ASPGE.pdf}
\caption{APDGE}
\label{Fig:ASGE}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=6cm]{APPGE.pdf}
\caption{APPGE}
\label{Fig:APGE}
\end{minipage}
\end{figure}


%======================================
\subsection{Adversarial Privacy-Purged Graph Embedding}
The adversarial training mechanism discussed above disentangles privacy latent factors from $\mathbf{Z}$ by disclosing the privacy labels directly as input to the decoder. We can also achieve a similar effect by disclosing the privacy labels as the output of the decoder. We call the latter mechanism ``Adversarial Privacy-Purged Graph Embedding (APPGE)", which is shown in Fig.~\ref{Fig:APGE}. This can be implemented by adopting the adversarial training framework similar as GAN. APPGE consists of two networks (1) An Attacker  whose purpose is to extract privacy information from embedding matrix $\mathbf{Z}$; and (2) An Obfuscator which attempts to embed utility information and prevent the inference attacks launched by the attacker.

%\begin{figure}
%  \centering
%  %\includegraphics[width=0.5\linewidth]{model2.eps}
%  \includegraphics[width=0.5\linewidth]{APGE.pdf}
%  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%  \caption{Adversarial Purging Privacy Graph Embedding}
%  \label{Fig:APGE}
%\end{figure}

Essentially, Attacker is a classifier that aims to predict privacy labels $\mathbf{Y}^{(p)}$. The architecture of Obfuscator is similar to GAE. The Obfuscator learns the latent code via GCN and decodes latent code to reconstruct network structure and utility attributes. In addition, it also trys to perturb the latent code such that teh attacker can't predict private attribute accurately. Therefore, we optimize the Obfuscator to minimize the reconstruction error and at the same time maximize the prediction error of the attacker. So the loss of the Obfuscator is as follows:
\begin{equation}
L_{obf} = L_{recon} - L_{att},
\label{Eq:obf}
\end{equation}
with
\begin{equation}
L_{att} =  -\frac{1}{|\mathbf{V}^{(p)}|}\sum_{i \in \mathbf{V}^{(p)}}\sum_{j=1}^{M_p}\mathbf{y}^{(p)}_{ij}\log\mathbf{\hat{y}}^{(p)}_{ij},
\end{equation}
where $\mathbf{V}^{(p)}$ is the set of users who own private attributes and $M_p$ is the dimension of private attribute label.
When we train the model, we update the attacker and obfuscator alternately until  both modules' loss functions are convergent.


%======================================
\subsection{Adversarial Privacy Graph Embedding}
The disentangling and purging mechanisms we discussed above preserve the privacy information at the different stages of the training: the former is at the input of the decoder, while the latter at the output of the decoder, and they work complementary to each other. Therefore, we integrate both mechanisms into one framework ``Adversarial Privacy Graph Embedding (APGE)" to jointly protect users' privacy, with the model shown in Fig. \ref{Fig:APDGE}. On the one hand, the framework of APGE is the same as APPGE that consists of an obfuscator and an attacker. On the other hand, the architecture of the obfuscator is the same as APDGE. Significantly, the attacker launches an inference attack on the latent code matrix $\mathbf{Z}$ rather than the latent code $\mathbf{Z^+}$, which is the concatenation of $Z$ and privacy label encoding.

The attacker and obfuscator of APGE can be optimized by alternating SGD in two stages. Firstly, for the attacker, we minimize the prediction error of the private attribute by Eq.~\ref{Eq:LossAttr}. Secondly, for the obfuscator, we update the model in three steps as shown in APDGE.

%The train steps are the same as the ones of APGE, Obfuscator and Attacker are trained alternately. When we train the Obfuscator, the process is the same as three stages of ASGE training, which we mentioned in section 4.1.  Significantly, the loss function of training encoder and decoder in step(b) is changed. Because when we train the encoder and decoder, we should consider minimizing the reconstruction error and maximizing the attacker prediction error both. So the loss of training encoder and decoder is the same as Eq. \ref{Eq:obf}.

\begin{figure}
  \centering
  \includegraphics[width=0.5\linewidth]{APGE.pdf}
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Adversarial Privacy Graph Embedding}
  \label{Fig:APDGE}
\end{figure}



%======================================
\section{Experiments}

%======================================
\subsection{Dataset and Baseline Methods}
We conduct experiments on 2 real-world social network datasets:
\textbf{Yale} and \textbf{Rochester}~\footnote{https://escience.rpi.edu/data/DA/fb100/} are subsets of Facebook collecting from the users of Yale University and Rochester University, respectively~\citep{traud2012social}. Specifically, \textbf{Yale} contains 8578 nodes and 405450 edges, \textbf{Rochester} contains 4563 nodes and 167653 edges, and both two datasets contain 7 attributes including student/faculty status, gender, major, second major, class year, dorm/house, and high school. We utilize 188-dimensional and 236-dimensional binary vector to respectively represent the users' attributes for \textbf{Yale} and \textbf{Rochester}. For \textbf{Yale}, class year (6 categories) is set as the sensitive attribute, and student/faculty status and gender are set as the utility attributes; while for \textbf{Rochester}, gender (2 categories) is set to be the sensitive attribute, and student/faculty status and class year are set to be the utility attributes. Indeed, our methods can also deal with the cases where other attributes as private, {\em e.g.,} sexual orientation and political opinion.%, if the values of sensitive attribute are provided.


The following baselines are adopted in the performance comparison.
(1) \textbf{GAE} \citep{kipf2016variational} learns network representation by autoencoder where encoder is graph convolutional network and decoder consists of inner product to reconstruct adjacency matrix and softmax function to predict utility attributes.
(2) \textbf{GAE\_RM} \citep{kipf2016variational} has a framework the same as that of GAE. Since GAE\_RM does not consider privacy protection, the columns corresponding to the sensitive attributes should be removed from the input feature matrix $\mathbf{X}$.
(3) \textbf{CDSPIA} \citep{cai2018collective} is a collective data-sanitization method to resist inference attacks by deleting or perturbing users' attributes and linkages which are closely related to privacy. We use CDSPIA to sanitize the social network dataset and then embed the graph data processed via GAE.


All the methods embed the social networks to a 64-dimensional space. Particularly, in APDGE and APGE, we first compress each node's information to 16-dimensional and 8-dimensional vector $\mathbf{z}'_i$ on Yale and Rochester, respectively.




%======================================
\subsection{Utility and Privacy Evaluation}
We measure the performance of the proposed models in terms of utility and privacy.


Utility is computed as the prediction accuracy of link and utility attributes from the embedding results of Yale and Rochester social network datasets.
For link prediction, 85\% edges are used as the training set, and 5\% and 10\% edges are respectively used as the true samples of validation set and the test set. Also, the same size false samples of the validation set and the test set are constructed by randomly selecting pairs of unconnected nodes.
According to Eq.~\ref{Eq:innerP}, the probability of edge existence is computed to distinguish the true or false samples. The area under the ROC curve (AUC) and average precision (AP) of link prediction of Yale and Rochester are shown  in Table~\ref{Tab:Yale} and Table~\ref{Tab:Rochester}.
For utility attribute prediction, multi-layer perceptron (MLP) is utilized to predict the utility attributes, and the 5-fold cross-validation accuracy of utility attribute prediction is reported in Table~\ref{Tab:Yale} and Table~\ref{Tab:Rochester}.



Three mainstream classifiers, including logistic regression (LR), MLP, and SVM, are employed as the attackers to predict the sensitive attribute, and 5-fold cross-validation is used to quantify the performance of preventing inference attack. The prediction accuracy of sensitive attribute on Yale and Rochester is shown in Table~\ref{Tab:Yale} and Table~\ref{Tab:Rochester}.


The performance of GAE\_RM indicates that even if the sensitive attribute is deleted from the published data, the attacker is still able to infer the user's privacy based on the inherent relation between the sensitive attribute and the released data.
CDSPIA can protect privacy more than GAE\_RM does but loses more utility information in its embedding results.
Compared with CDSPIA, the ability of APDGE and APPGE to protect privacy is comparable while the ability of APDGE and APPGE to capture link and utility attribute is better.
From the comparison, APPGE is more suitable to preventing LR attack, APDGE can defend against the attack via MLP and SVM better.  APGE which is the integration of APPGE and APDGE owns the strongest ability to protect user's privacy.  With the embedding result of APGE, the prediction accuracy of private attribute of {\bf Yale} is reduced significantly, and the prediction accuracy of private attribute of {\bf Rochester} is close to that of random-guess. Meanwhile, the ability of APGE capturing the link and utility attributes information does not decrease significantly.
%
\begin{table}
\small
  \caption{Utility and Privacy Evaluation on Yale}
  \label{Tab:Yale}
  \centering
  \begin{tabular}{lcccccccc}
    \toprule
     Method & \multicolumn{2}{c}{link} & \multicolumn{2}{c}{utility attributes} &\multicolumn{3}{c}{private attribute(class year)}                  \\
    \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-8}
       & AUC  &AP     &status &gender  &LR  &MLP &SVM  \\
    \midrule
    GAE & 93.1 $\pm$ 0.2 &93.6$\pm$ 0.2     &94.3 $\pm$0.3  &91.9$\pm$ 0.4  &94.7 $\pm$0.1 &94.2$\pm$  0.2 &94.0$\pm$ 0.3\\
    GAE\_RM    &93.0 $\pm$ 0.1 &93.4$\pm$ 0.1  &94.2 $\pm$0.2   &91.6$\pm$ 0.4  &91.0$\pm$ 0.2 &91.0 $\pm$0.2 &91.0$\pm$ 0.1   \\
    CDSPIA     &90.4 $\pm$  0.1    &90.0 $\pm$0.2  &94.2$\pm$ 0.1  &87.3$\pm$ 0.4 &87.9$\pm$ 0.2 &90.2 $\pm$0.2 &88.7$\pm$ 0.3 \\
    APDGE     &91.4 $\pm$  0.2     & 91.4 $\pm$0.2  &92.5$\pm$ 0.4  &90.8$\pm$ 0.6 &81.3$\pm$ 0.6  &83.9$\pm$ 0.7 &73.7$\pm$ 0.7 \\
    APPGE    &92.7 $\pm$  0.2   &92.9 $\pm$0.2  &93.7$\pm$ 0.1  &90.2$\pm$ 0.2 &79.4$\pm$ 1.4 &88.7$\pm$ 0.2 &82.9$\pm$ 0.7\\
    APGE     &88.6   $\pm $0.5  &88.6$\pm$ 0.6  &83.8$\pm$ 0.9  &88.3$\pm$ 0.8 &\textbf{41.4$\pm$ 1.6} &\textbf{53.4$\pm$ 2.0} &\textbf{38.6$\pm$ 1.3}\\
    \bottomrule
  \end{tabular}
\end{table}
%
\begin{table}
\small
  \caption{Utility and Privacy Evaluation on Rochester}
  \label{Tab:Rochester}
  \centering
  \begin{tabular}{lcccccccc}
    \toprule
     Method & \multicolumn{2}{c}{link} & \multicolumn{2}{c}{utility attributes} &\multicolumn{3}{c}{private attribute(gender)}                  \\
    \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-8}
       & AUC  &AP     &status &class year  &LR  &MLP &SVM  \\
    \midrule
    GAE   &93.3 $\pm$ 0.2  &93.8 $\pm$ 0.3 &98.0 $\pm$ 0.2 &96.8 $\pm$ 0.3 &74.9 $\pm$ 0.9 &78.3 $\pm$ 0.3 &72.8 $\pm$ 1.0\\
    GAE\_RM  &93.6 $\pm$  0.1 &94.1 $\pm$ 0.1 &98.4 $\pm$ 0.5 &97.5 $\pm$  0.3 &67.5 $\pm$ 0.7 &72.1 $\pm$ 0.4 &67.4 $\pm$ 1.0\\
    CDSPIA  &92.2 $\pm$ 0.1 &92.3 $\pm$ 0.3 &96.3 $\pm$  0.3 &93.8 $\pm$ 0.2 &57.4 $\pm$ 0.7 &63.5 $\pm$ 0.4 &56.7 $\pm$ 1.3\\
    APDGE     &91.4 $\pm$ 0.3 &91.4 $\pm$ 0.3 &96.6 $\pm$ 0.5 &95.4 $\pm$ 0.3 &55.1 $\pm$ 0.7 &59.3  $\pm$ 0.4 &54.2 $\pm$ 0.2\\
    APPGE     &93.6 $\pm$ 0.2 &94.1 $\pm$ 0.2 &97.3  $\pm$ 0.7 &96.9 $\pm$ 0.2  &\textbf{48.2 $\pm$  0.2}   &70.5$\pm$  0.3 &55.2 $\pm$ 0.9\\
    APGE     &91.1  $\pm$ 0.2 &91.1 $\pm$ 0.2 &96.7 $\pm$ 0.3 &95.6 $\pm$ 0.4 &52.4 $\pm$ 0.6 &\textbf{59.2 $\pm$ 0.5} &\textbf{53.0 $\pm$ 0.4}\\
    \bottomrule
  \end{tabular}
\end{table}


Moreover, to illustrate the trade off between utility and privacy, the ratio of utility (that is the total prediction accuracy of link and utility attributes) to privacy is analyzed via Fig.~\ref{fig:u_p}. The results of "utility/privacy'' show that APGE obtains the best performance when both utility reserving and privacy protection are taken into account.
%
\begin{figure}[!ht]
     \subfloat[utilty/privacy on Yale\label{Classcial AAE}]{%
       \includegraphics[width=0.48\textwidth]{YaleU_P.pdf}
     }
     \hfill
     \subfloat[utilty/privacy on Rochestor\label{Supervised AAE}]{%
       \includegraphics[width=0.48\textwidth]{RochU_P.pdf}
     }
     \caption{evaluation on the trade off between utility and privacy}
     \label{fig:u_p}
   \end{figure}


%======================================
\subsection{Impact of Parameter}
With various dimensions of hidden code $\mathbf{z}'_i$ in APGE on \textbf{Yale}, the results of privacy and utility are presented in Fig.~\ref{fig:Impact}. When the dimension of hidden code is increased, more information of link and utility attribute are embedded. On the other hand, a higher dimension of hidden code also embeds more private information, leading to a lower probability of preventing inference attack successfully. Here, we define predicting false privacy attribute as preventing attack successfully.
%
\begin{figure}[!ht]
     \subfloat[Impact on Utility\label{subfig-1:dummy}]{%
       \includegraphics[width=0.48\textwidth]{impact_U.pdf}
     }
     \hfill
     \subfloat[Impact on privacy\label{subfig-2:dummy}]{%
       \includegraphics[width=0.48\textwidth]{impact_P.pdf}
     }
     \caption{Impact of the hidden code's dimension }
     \label{fig:Impact}
   \end{figure}




%======================================
\section{Related work}

\textbf{Graph Embedding Method.}\\
DeepWalk~\citep{perozzi2014deepwalk} is the early work to learn graph representation by generating the node context via random walk and mapping node to vectors based on skip-gram~\citep{mikolov2013efficient}.
Node2vec~\citep{grover2016node2vec} exploits a new method to generate node context with considering both local and global structure information and then embeds graph using DeepWalk.
The above methods just utilize topology information, and some works have been proposed to improve embedding performance taking into account both the structure and attribute information.
TirDNR~\citep{pan2016tri} combines DeepWalk and Doc2Vec~\citep{dai2015document} to learn the inner-node relationship, node-word correspondence, and word-label correlation.
UPP-SNE~\citep{zhang2017user} uses the attribute matrix of social network as input and the node context as label to supervised training model.
Different from skip-gram architecture, GAE~\citep{kipf2016variational} is a graph convolutional network via an approximation of spectral graph convolutions for learning the graph representation and outperforms skip-gram architecture.
Recently, ARGA~\citep{pan2018adversarially} is proposed to add regularization to GAE via an adversarial framework.
Unfortunately, these above graph embedding methods do not consider privacy preservation; that is, attacker can infer the user's sensitive attribute from the embedding results.



\textbf{Privacy Preservation in Social Network against Inference Attack.}\\
Jia and Gong~\citep{jia2018attriguard} build a set of noise via adversarial machine learning and randomly select the noise to mislead the inference attacker. However, they only manipulate the user's attribute and ignore the linkage, and thus their method cannot resist the inference attack utilizing topology information well.
Cai {\em et al.}~\citep{cai2018collective} first propose an inference attack method with the mixture of non-sensitive attribute and link relationship and then design a privacy-preserving method by removing or perturbing user's attributes and links that are closely related to private attribute. He {\em et al.}~\citep{he2018latent} obtain the data-sanitization strategy by solving an optimization problem, which can balance the trade off between utility and privacy.
In these prior works, privacy is preserved by directly manipulating graph data, which decouples the processes of privacy protection and graph embedding and is not suitable to implement privacy-preserving embedding on social networks.



%======================================
\section{Conclusion}
In this paper, we proposed a novel social network embedding framework, which can protect user's privacy (i.e., sensitive attribute) against the inference attack while remaining the user's utility information (including link and utility attribute). The proposed framework not only disentangles privacy information via  incorporating the privacy label to the decoder of the autoencoder architecture but also purging embedding vector to confuse attackers in adversarial network. Comprehensive experiment results validate that our proposed framework \textbf{APGE} outperforms the state-of-the-art in terms of social network privacy preserving embedding.

\bibliographystyle{unsrt}
\bibliography{bibfile}


\end{document}
